{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Low-Rank Adaptation\n\nThis notebook explores the use of Low-Rank Adaptation (LoRA) method of fine-tuning LLMs. LoRA is a parameter efficient fine tuning (PEFT) method which drastically reduces the number of trainable parameters when compared to full fine tuning. Typically the number of tuned parameters is 10% or less of the total number of parameters in the LLM. LoRA provides some other advantages:\n* LoRA fine-tuned models do not experience additional inference latency\n* The original model weights are left unchanged so that multiple LoRA adapters can be trained for different tasks.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"%pip install \\\n    torch \\\n    transformers \\\n    datasets \\\n    evaluate \\\n    rouge_score \\\n    loralib \\\n    peft --quiet","metadata":{"execution":{"iopub.status.busy":"2024-01-12T07:23:33.907937Z","iopub.execute_input":"2024-01-12T07:23:33.908281Z","iopub.status.idle":"2024-01-12T07:23:49.816323Z","shell.execute_reply.started":"2024-01-12T07:23:33.908253Z","shell.execute_reply":"2024-01-12T07:23:49.815042Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The next two cells handle issues with Kaggle environment.","metadata":{}},{"cell_type":"code","source":"%pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-01-12T07:23:55.269023Z","iopub.execute_input":"2024-01-12T07:23:55.269766Z","iopub.status.idle":"2024-01-12T07:24:09.226374Z","shell.execute_reply.started":"2024-01-12T07:23:55.269732Z","shell.execute_reply":"2024-01-12T07:24:09.225240Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.1 fsspec-2023.10.0 pyarrow-hotfix-0.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb offline","metadata":{"execution":{"iopub.status.busy":"2024-01-12T07:24:13.228108Z","iopub.execute_input":"2024-01-12T07:24:13.228502Z","iopub.status.idle":"2024-01-12T07:24:15.594430Z","shell.execute_reply.started":"2024-01-12T07:24:13.228468Z","shell.execute_reply":"2024-01-12T07:24:15.593327Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\nimport evaluate\nimport pandas as pd\nimport torch\n\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2024-01-12T07:26:45.786896Z","iopub.execute_input":"2024-01-12T07:26:45.787327Z","iopub.status.idle":"2024-01-12T07:27:03.873314Z","shell.execute_reply.started":"2024-01-12T07:26:45.787292Z","shell.execute_reply":"2024-01-12T07:27:03.872530Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset and problem setup\n\nWe will use the [databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset from Databricks. It is an open source dataset containing records from various including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization. We are interested in the 'closed QA' portion of the dataset. Briefly closed QA involves answering a question in the context of information given in a passage of text. Let us observe some sample records.","metadata":{}},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"DATASET_NAME = 'databricks/databricks-dolly-15k'\nRNG_SEED=10\n\noriginal_dataset = load_dataset(DATASET_NAME, split='train')\ndataset = original_dataset.shuffle(seed=RNG_SEED).filter(lambda example: example['category']=='closed_qa')\n\nfor i in range(3):\n    print(''.join(['-'] * 80))\n    print('CONTEXT:')\n    print(dataset[i]['context'])\n    print('\\n\\n')\n    print('INSTRUCTION:')\n    print(dataset[i]['instruction'])\n    print('\\n\\n')\n    print('RESPONSE:')\n    print(dataset[i]['response'])","metadata":{"execution":{"iopub.status.busy":"2024-01-12T07:42:10.592461Z","iopub.execute_input":"2024-01-12T07:42:10.593167Z","iopub.status.idle":"2024-01-12T07:42:11.591214Z","shell.execute_reply.started":"2024-01-12T07:42:10.593126Z","shell.execute_reply":"2024-01-12T07:42:11.590243Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"--------------------------------------------------------------------------------\nCONTEXT:\nWoodstock Music and Art Fair, commonly referred to as Woodstock, was a music festival held during August 15–18, 1969, on Max Yasgur's dairy farm in Bethel, New York, United States, 40 miles (65 km) southwest of the town of Woodstock. Billed as \"an Aquarian Exposition: 3 Days of Peace & Music\" and alternatively referred to as the Woodstock Rock Festival, it attracted an audience of more than 400,000 attendees. Thirty-two acts performed outdoors despite sporadic rain. It was one of the largest music festivals held in history.\n\n\n\nINSTRUCTION:\nDid the Grateful Dead play at the original Woodstock concert?\n\n\n\nRESPONSE:\nYes, the Grateful Dead played a 1 hour and 35 minute set on Saturday, August 16 1969, that ended after a fifty-minute version of \"Turn On Your Love Light\".\n--------------------------------------------------------------------------------\nCONTEXT:\nIn the United States, a 401(k) plan is an employer-sponsored, defined-contribution, personal pension (savings) account, as defined in subsection 401(k) of the U.S. Internal Revenue Code. Periodical employee contributions come directly out of their paychecks, and may be matched by the employer. This legal option is what makes 401(k) plans attractive to employees, and many employers offer this option to their (full-time) workers.\n\nThere are two types: traditional and Roth 401(k). For Roth accounts, contributions and withdrawals have no impact on income tax. For traditional accounts, contributions may be deducted from taxable income and withdrawals are added to taxable income. There are limits to contributions, rules governing withdrawals and possible penalties.\n\nThe benefit of the Roth account is from tax-free capital gains. The net benefit of the traditional account is the sum of (1) a possible bonus (or penalty) from withdrawals at tax rates lower (or higher) than at contribution, and (2) the impact on qualification for other income-tested programs from contributions and withdrawals reducing and adding to taxable income, minus the consequences of capital gains being taxed at regular income rates.\n\n\n\nINSTRUCTION:\nWhat is a Roth 401(k)?\n\n\n\nRESPONSE:\nRoth 401(k) is a type of 401(k) where accounts, contributions, and withdrawals have no impact on income tax. The benefit of the Roth account is that the capital gains are tax-free.\n\nIn the United States, a 401(k) plan is an employer-sponsored, defined-contribution, personal pension (savings) account, as defined in subsection 401(k) of the U.S. Internal Revenue Code. Periodical employee contributions come directly out of their paychecks and may be matched by the employer.\n--------------------------------------------------------------------------------\nCONTEXT:\nNatural hydrogen (known as white hydrogen), is naturally occurring molecular hydrogen on or in Earth (as opposed to hydrogen produced in the laboratory or in industry). The name white hydrogen distinguishes it from green hydrogen, which is produced from renewable energy sources, and from grey, brown or black hydrogen, which is obtained from fossil sources or from the electrolysis of water. Natural hydrogen may be renewable, non-polluting and allows for lower cost operation compared to industrial hydrogen. Natural hydrogen has been identified in many source rocks in areas beyond the sedimentary basins where oil companies typically operate.\n\nOrigin of natural hydrogen\nThere are several sources of natural hydrogen:\n\n- degassing of deep hydrogen from the Earth's crust and mantle;\n- reaction of water with ultrabasic rocks (serpentinisation);\n- contact of water with reducing agents in the Earth's mantle;\n- interaction of water with freshly exposed rock surfaces (weathering);\n- decomposition of hydroxyl ions in the structure of minerals;\n- Natural radiolysis of water;\n- decomposition of organic matter;\n- biological activity\n- Extraction\n- Natural hydrogen is extracted from wells, mixed with other gases such as nitrogen or helium.\n\nSeveral sources have been identified in France. Geologists Alain Prinzhofer and Eric Derville have demonstrated the existence of large reservoirs in a dozen countries, including Mali and the United States. However, their potential remains difficult to assess.\n\nNumerous emanations on the ocean floor have been identified but are difficult to exploit. The discovery of a significant emergence in Russia in 2008 suggests the possibility of extracting native hydrogen in geological environments.\n\nGeology\nNatural hydrogen is generated continuously from a variety of natural sources. There are many known hydrogen emergences on mid-ocean ridges. Another of the known reactions, serpentinisation, occurs under the sea floor (in the oceanic crust).\n\nDiagenetic origin (iron oxidation) in the sedimentary basins of cratons, notably in Russia. Other sources are being explored, such as mantle hydrogen, or hydrogen from radiolysis (natural electrolysis) or from bacterial activity. In France, the Alps and Pyrenees are suitable for exploitation. New Caledonia has hyperalkaline sources that show dihydrogen emissions. A large accumulation of natural hydrogen was discovered in Bourakebougou (Mali).\n\nCharacteristics\nDihydrogen is very soluble in fresh water, especially at depth (solubility increases with pressure).\n\nhttps://en.wikipedia.org/wiki/Natural_hydrogen\n\n\n\nINSTRUCTION:\nGiven these paragraphs about Natural hydrogen, what is it?\n\n\n\nRESPONSE:\nNatural hydrogen (known as white hydrogen), is naturally occurring molecular hydrogen on or in Earth (as opposed to hydrogen produced in the laboratory or in industry).\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a 60-20-20 train/test/validation split of the dataset\ntrain_testvalid = dataset.train_test_split(test_size=0.4)\ntest_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T07:41:13.077111Z","iopub.execute_input":"2024-01-12T07:41:13.077625Z","iopub.status.idle":"2024-01-12T07:41:13.100078Z","shell.execute_reply.started":"2024-01-12T07:41:13.077585Z","shell.execute_reply":"2024-01-12T07:41:13.099042Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\nWe will use the [google/flan-t5-small](https://huggingface.co/google/flan-t5-small) model. This is a small model with only 77M parameters which can perform summarization, question answering, sentence completion, word sense disambiguation and other tasks. We will first evaluate the pre-trained model on closed qa task and try to improve its performance using LoRA fine tuning.","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = 'google/flan-t5-small'\nDEVICE = torch.device('cuda') if torch.cuda.is_available() == True else torch.device('cpu')\nDTYPE = torch.bfloat16\npre_trained_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(DEVICE)\nprint(f'Pre trained model tensor datatype: {pre_trained_model.dtype}')\nprint(f'Pre trained model device: {pre_trained_model.device}')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T07:51:08.001190Z","iopub.execute_input":"2024-01-12T07:51:08.002272Z","iopub.status.idle":"2024-01-12T07:51:09.474723Z","shell.execute_reply.started":"2024-01-12T07:51:08.002231Z","shell.execute_reply":"2024-01-12T07:51:09.473700Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Pre trained model tensor datatype: torch.bfloat16\nPre trained model device: cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenizer\n\nBefore we can inspect how the model performs we need to create a tokenizer that works with the model.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nprompt_template = \"\"\"{context}\n\nGiven the above passage, answer the following question:\n\n{instruction}\n\"\"\"\n\nfor i in range(3):\n    print(''.join(['-'] * 80))\n    print('CONTEXT:')\n    print(dataset[i]['context'])\n    print('\\n\\n')\n    print('INSTRUCTION:')\n    print(dataset[i]['instruction'])\n    print('\\n\\n')\n    print('RESPONSE:')\n    print(dataset[i]['response'])\n    print('\\n\\n')\n    print('FLAN-T5-RESPONSE:')\n    inputs = tokenizer(\n        prompt_template.format(\n            context=dataset[i]['context'],\n            instruction=dataset[i]['instruction'],\n        ),\n        return_tensors='pt',\n        truncation=True,\n        max_length=512\n    ).to(DEVICE)\n    output = tokenizer.decode(\n        pre_trained_model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=200,\n        )[0], \n        skip_special_tokens=True\n    )\n    print(output)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T08:03:12.527982Z","iopub.execute_input":"2024-01-12T08:03:12.528399Z","iopub.status.idle":"2024-01-12T08:03:13.137193Z","shell.execute_reply.started":"2024-01-12T08:03:12.528369Z","shell.execute_reply":"2024-01-12T08:03:13.136133Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"--------------------------------------------------------------------------------\nCONTEXT:\nWoodstock Music and Art Fair, commonly referred to as Woodstock, was a music festival held during August 15–18, 1969, on Max Yasgur's dairy farm in Bethel, New York, United States, 40 miles (65 km) southwest of the town of Woodstock. Billed as \"an Aquarian Exposition: 3 Days of Peace & Music\" and alternatively referred to as the Woodstock Rock Festival, it attracted an audience of more than 400,000 attendees. Thirty-two acts performed outdoors despite sporadic rain. It was one of the largest music festivals held in history.\n\n\n\nINSTRUCTION:\nDid the Grateful Dead play at the original Woodstock concert?\n\n\n\nRESPONSE:\nYes, the Grateful Dead played a 1 hour and 35 minute set on Saturday, August 16 1969, that ended after a fifty-minute version of \"Turn On Your Love Light\".\n\n\n\nFLAN-T5-RESPONSE:\nyes\n--------------------------------------------------------------------------------\nCONTEXT:\nIn the United States, a 401(k) plan is an employer-sponsored, defined-contribution, personal pension (savings) account, as defined in subsection 401(k) of the U.S. Internal Revenue Code. Periodical employee contributions come directly out of their paychecks, and may be matched by the employer. This legal option is what makes 401(k) plans attractive to employees, and many employers offer this option to their (full-time) workers.\n\nThere are two types: traditional and Roth 401(k). For Roth accounts, contributions and withdrawals have no impact on income tax. For traditional accounts, contributions may be deducted from taxable income and withdrawals are added to taxable income. There are limits to contributions, rules governing withdrawals and possible penalties.\n\nThe benefit of the Roth account is from tax-free capital gains. The net benefit of the traditional account is the sum of (1) a possible bonus (or penalty) from withdrawals at tax rates lower (or higher) than at contribution, and (2) the impact on qualification for other income-tested programs from contributions and withdrawals reducing and adding to taxable income, minus the consequences of capital gains being taxed at regular income rates.\n\n\n\nINSTRUCTION:\nWhat is a Roth 401(k)?\n\n\n\nRESPONSE:\nRoth 401(k) is a type of 401(k) where accounts, contributions, and withdrawals have no impact on income tax. The benefit of the Roth account is that the capital gains are tax-free.\n\nIn the United States, a 401(k) plan is an employer-sponsored, defined-contribution, personal pension (savings) account, as defined in subsection 401(k) of the U.S. Internal Revenue Code. Periodical employee contributions come directly out of their paychecks and may be matched by the employer.\n\n\n\nFLAN-T5-RESPONSE:\nan employer-sponsored, defined-contribution, personal pension (savings) account\n--------------------------------------------------------------------------------\nCONTEXT:\nNatural hydrogen (known as white hydrogen), is naturally occurring molecular hydrogen on or in Earth (as opposed to hydrogen produced in the laboratory or in industry). The name white hydrogen distinguishes it from green hydrogen, which is produced from renewable energy sources, and from grey, brown or black hydrogen, which is obtained from fossil sources or from the electrolysis of water. Natural hydrogen may be renewable, non-polluting and allows for lower cost operation compared to industrial hydrogen. Natural hydrogen has been identified in many source rocks in areas beyond the sedimentary basins where oil companies typically operate.\n\nOrigin of natural hydrogen\nThere are several sources of natural hydrogen:\n\n- degassing of deep hydrogen from the Earth's crust and mantle;\n- reaction of water with ultrabasic rocks (serpentinisation);\n- contact of water with reducing agents in the Earth's mantle;\n- interaction of water with freshly exposed rock surfaces (weathering);\n- decomposition of hydroxyl ions in the structure of minerals;\n- Natural radiolysis of water;\n- decomposition of organic matter;\n- biological activity\n- Extraction\n- Natural hydrogen is extracted from wells, mixed with other gases such as nitrogen or helium.\n\nSeveral sources have been identified in France. Geologists Alain Prinzhofer and Eric Derville have demonstrated the existence of large reservoirs in a dozen countries, including Mali and the United States. However, their potential remains difficult to assess.\n\nNumerous emanations on the ocean floor have been identified but are difficult to exploit. The discovery of a significant emergence in Russia in 2008 suggests the possibility of extracting native hydrogen in geological environments.\n\nGeology\nNatural hydrogen is generated continuously from a variety of natural sources. There are many known hydrogen emergences on mid-ocean ridges. Another of the known reactions, serpentinisation, occurs under the sea floor (in the oceanic crust).\n\nDiagenetic origin (iron oxidation) in the sedimentary basins of cratons, notably in Russia. Other sources are being explored, such as mantle hydrogen, or hydrogen from radiolysis (natural electrolysis) or from bacterial activity. In France, the Alps and Pyrenees are suitable for exploitation. New Caledonia has hyperalkaline sources that show dihydrogen emissions. A large accumulation of natural hydrogen was discovered in Bourakebougou (Mali).\n\nCharacteristics\nDihydrogen is very soluble in fresh water, especially at depth (solubility increases with pressure).\n\nhttps://en.wikipedia.org/wiki/Natural_hydrogen\n\n\n\nINSTRUCTION:\nGiven these paragraphs about Natural hydrogen, what is it?\n\n\n\nRESPONSE:\nNatural hydrogen (known as white hydrogen), is naturally occurring molecular hydrogen on or in Earth (as opposed to hydrogen produced in the laboratory or in industry).\n\n\n\nFLAN-T5-RESPONSE:\nNatural radiolysis of water\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see the pre-trained model gives generally correct but terse responses. When evaluated on a metric like 'rouge' it would have a low score due to low recall.","metadata":{}},{"cell_type":"markdown","source":"## Performance of pre-trained model\n\nNext we evaluate the pre-trained model on the 'test' portion of the dataset and compute the 'rouge' evaluation metric.","metadata":{}},{"cell_type":"code","source":"contexts = test_valid['test']['context']\ninstructions = test_valid['test']['instruction']\nresponses = test_valid['test']['response']\n\npretrained_model_responses = []\nstart_time = time.time()\nfor context, instruction in zip(contexts, instructions):\n    input_ids = tokenizer(\n        prompt_template.format(context=context, instruction=instruction),\n        return_tensors='pt',\n        truncation=True,\n        max_length=512,\n    ).to(DEVICE).input_ids\n    pretrained_model_output = pre_trained_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    pretrained_model_response = tokenizer.decode(pretrained_model_output[0], skip_special_tokens=True)\n    pretrained_model_responses.append(pretrained_model_response)\nend_time = time.time()\nprint(f\"Time taken for inference on {test_valid['test'].num_rows} examples: {end_time - start_time}\")\nzipped_responses = list(zip(responses, pretrained_model_responses))\ndf = pd.DataFrame(zipped_responses, columns=['reference_response', 'pretrained_model_response'])","metadata":{"execution":{"iopub.status.busy":"2024-01-12T08:48:46.198735Z","iopub.execute_input":"2024-01-12T08:48:46.199728Z","iopub.status.idle":"2024-01-12T08:49:28.068865Z","shell.execute_reply.started":"2024-01-12T08:48:46.199691Z","shell.execute_reply":"2024-01-12T08:49:28.067892Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Time taken for inference on 355 examples: 41.84682536125183\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge = evaluate.load('rouge')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T08:49:41.018384Z","iopub.execute_input":"2024-01-12T08:49:41.019443Z","iopub.status.idle":"2024-01-12T08:49:41.573776Z","shell.execute_reply.started":"2024-01-12T08:49:41.019401Z","shell.execute_reply":"2024-01-12T08:49:41.572768Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"pretrained_model_results = rouge.compute(\n    predictions=pretrained_model_responses,\n    references=responses,\n    use_aggregator=True,\n    use_stemmer=True,\n)\nprint(pretrained_model_results)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T08:49:42.271070Z","iopub.execute_input":"2024-01-12T08:49:42.271640Z","iopub.status.idle":"2024-01-12T08:49:43.342263Z","shell.execute_reply.started":"2024-01-12T08:49:42.271597Z","shell.execute_reply":"2024-01-12T08:49:43.341203Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"{'rouge1': 0.22349003544929158, 'rouge2': 0.11017409587228838, 'rougeL': 0.21701084748381727, 'rougeLsum': 0.2171873334906056}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As discussed earlier, the performance of the pre-trained model is not great due to the fact that it outputs extremely brief responses.","metadata":{}},{"cell_type":"markdown","source":"## Fine tuning the model\n\nNext we will use the LoRA technique to fine tune the model on the validation dataset for a range of ranks and see if it improves the rouge score. First we will tokenize the dataset to speed up training.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(example):\n    prompt = [\n        prompt_template.format(context=context, instruction=instruction) \n        for context, instruction in zip(example[\"context\"], example[\"instruction\"])\n    ]\n    example['input_ids'] = tokenizer(\n        prompt, \n        padding=\"max_length\", \n        truncation=True, \n        max_length=512, \n        return_tensors=\"pt\"\n    ).to(DEVICE).input_ids\n    example['labels'] = tokenizer(\n        example[\"response\"], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=512, \n        return_tensors=\"pt\"\n    ).to(DEVICE).input_ids\n    \n    return example","metadata":{"execution":{"iopub.status.busy":"2024-01-12T08:36:28.513154Z","iopub.execute_input":"2024-01-12T08:36:28.514165Z","iopub.status.idle":"2024-01-12T08:36:28.521093Z","shell.execute_reply.started":"2024-01-12T08:36:28.514125Z","shell.execute_reply":"2024-01-12T08:36:28.519972Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"tokenized_train_testvalid = train_testvalid.map(tokenize_function, batched=True)\ntokenized_test_valid = test_valid.map(tokenize_function, batched=True)\ntokenized_train_testvalid = tokenized_train_testvalid.remove_columns(['instruction', 'context', 'response', 'category',])\ntokenized_test_valid = tokenized_test_valid.remove_columns(['instruction', 'context', 'response', 'category',])","metadata":{"execution":{"iopub.status.busy":"2024-01-12T08:36:31.222513Z","iopub.execute_input":"2024-01-12T08:36:31.223281Z","iopub.status.idle":"2024-01-12T08:36:32.211321Z","shell.execute_reply.started":"2024-01-12T08:36:31.223244Z","shell.execute_reply":"2024-01-12T08:36:32.210537Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/710 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"686798b8a7644f06896371c54e92f0c1"}},"metadata":{}}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T09:00:22.359607Z","iopub.execute_input":"2024-01-12T09:00:22.360327Z","iopub.status.idle":"2024-01-12T09:00:22.366031Z","shell.execute_reply.started":"2024-01-12T09:00:22.360292Z","shell.execute_reply":"2024-01-12T09:00:22.364917Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nranks_to_try = [1, 2, 4, 8]\npeft_model_paths = []\n\nfor rank in ranks_to_try:\n    print(''.join(['-'] * 80))\n    print(f'Rank: {rank}')\n    lora_config = LoraConfig(\n        r=rank, # Rank\n        lora_alpha=32,\n        target_modules=[\"q\", \"v\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n    )\n    peft_model = get_peft_model(pre_trained_model, lora_config)\n    print(print_number_of_trainable_model_parameters(peft_model))\n    peft_training_args = TrainingArguments(\n        output_dir=f'./peft-training-{rank}-{str(int(time.time()))}',\n        auto_find_batch_size=True,\n        learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n        num_train_epochs=10,\n        logging_steps=250,\n    )\n    peft_trainer = Trainer(\n        model=peft_model,\n        args=peft_training_args,\n        train_dataset=tokenized_train_testvalid[\"train\"],\n    )\n    peft_trainer.train()\n    peft_model_path=f\"./peft-checkpoint-{rank}-{str(int(time.time()))}\"\n    peft_trainer.model.save_pretrained(peft_model_path)\n    tokenizer.save_pretrained(peft_model_path)\n    peft_model_paths.append(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T09:00:24.493899Z","iopub.execute_input":"2024-01-12T09:00:24.494293Z","iopub.status.idle":"2024-01-12T09:39:34.062747Z","shell.execute_reply.started":"2024-01-12T09:00:24.494260Z","shell.execute_reply":"2024-01-12T09:39:34.061737Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"--------------------------------------------------------------------------------\nRank: 1\ntrainable model parameters: 43008\nall model parameters: 77004160\npercentage of trainable model parameters: 0.06%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1330/1330 08:50, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>5.561600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.994600</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.932900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.912700</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.916100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"--------------------------------------------------------------------------------\nRank: 2\ntrainable model parameters: 86016\nall model parameters: 77047168\npercentage of trainable model parameters: 0.11%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1330/1330 09:51, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>5.658800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.028800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.953900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.932400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.935000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"--------------------------------------------------------------------------------\nRank: 4\ntrainable model parameters: 172032\nall model parameters: 77133184\npercentage of trainable model parameters: 0.22%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1330/1330 09:51, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>5.578500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.041900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.958800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.925200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.926100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"--------------------------------------------------------------------------------\nRank: 8\ntrainable model parameters: 344064\nall model parameters: 77305216\npercentage of trainable model parameters: 0.45%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1330/1330 09:53, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>5.680400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.066400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.985100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.956600</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.957900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"test_valid","metadata":{"execution":{"iopub.status.busy":"2024-01-12T09:41:58.031892Z","iopub.execute_input":"2024-01-12T09:41:58.032610Z","iopub.status.idle":"2024-01-12T09:41:58.040564Z","shell.execute_reply.started":"2024-01-12T09:41:58.032576Z","shell.execute_reply":"2024-01-12T09:41:58.039389Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'context', 'response', 'category'],\n        num_rows: 355\n    })\n    test: Dataset({\n        features: ['instruction', 'context', 'response', 'category'],\n        num_rows: 355\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\n# peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16).to(DEVICE)\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ncontexts = test_valid['train']['context']\ninstructions = test_valid['train']['instruction']\nresponses = test_valid['train']['response']\n\nrouge1_results = []\nrouge2_results = []\nrougeL_results = []\nrougeLsum_results = []\n\nfor path in peft_model_paths:\n    # instantiate peft model\n    peft_saved_model = PeftModel.from_pretrained(\n        pre_trained_model, \n        path, \n        torch_dtype=torch.bfloat16,\n        is_trainable=False\n    ).to(DEVICE)\n\n    peft_model_responses = []\n    start_time = time.time()\n    for context, instruction in zip(contexts, instructions):\n        input_ids = tokenizer(\n            prompt_template.format(context=context, instruction=instruction),\n            return_tensors='pt',\n            truncation=True,\n            max_length=512,\n        ).to(DEVICE).input_ids\n        peft_model_output = peft_saved_model.generate(\n            input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)\n        )\n        peft_model_response = tokenizer.decode(peft_model_output[0], skip_special_tokens=True)\n        peft_model_responses.append(peft_model_response)\n    end_time = time.time()\n    print(f\"Time taken for inference on {test_valid['train'].num_rows} examples: {end_time - start_time}\")\n    peft_model_result = rouge.compute(\n        predictions=peft_model_responses,\n        references=responses,\n        use_aggregator=True,\n        use_stemmer=True,\n    )\n    rouge1_results.append(peft_model_result['rouge1'])\n    rouge2_results.append(peft_model_result['rouge2'])\n    rougeL_results.append(peft_model_result['rougeL'])\n    rougeLsum_results.append(peft_model_result['rougeLsum'])\n\nzipped_results = list(zip(ranks_to_try, rouge1_results, rouge2_results, rougeL_results, rougeLsum_results))\nresults_df = pd.DataFrame(zipped_results, columns=['rank', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum'])\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-01-12T09:48:50.318052Z","iopub.execute_input":"2024-01-12T09:48:50.318483Z","iopub.status.idle":"2024-01-12T10:00:17.930717Z","shell.execute_reply.started":"2024-01-12T09:48:50.318452Z","shell.execute_reply":"2024-01-12T10:00:17.929629Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Time taken for inference on 355 examples: 201.056640625\nTime taken for inference on 355 examples: 176.9531762599945\nTime taken for inference on 355 examples: 176.1388807296753\nTime taken for inference on 355 examples: 126.36030602455139\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"   rank    rouge1    rouge2    rougeL  rougeLsum\n0     1  0.340621  0.204046  0.315200   0.314422\n1     2  0.353333  0.206454  0.324178   0.324214\n2     4  0.358939  0.215746  0.331947   0.331981\n3     8  0.327430  0.191774  0.308966   0.308135","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rank</th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.340621</td>\n      <td>0.204046</td>\n      <td>0.315200</td>\n      <td>0.314422</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.353333</td>\n      <td>0.206454</td>\n      <td>0.324178</td>\n      <td>0.324214</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0.358939</td>\n      <td>0.215746</td>\n      <td>0.331947</td>\n      <td>0.331981</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>0.327430</td>\n      <td>0.191774</td>\n      <td>0.308966</td>\n      <td>0.308135</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"There is no appreciable difference in the performance of the four models -- although the model tuned with rank=4 seems to have a slight edge over the others. We will no evaluate its performance on the test dataset.","metadata":{}},{"cell_type":"code","source":"contexts = test_valid['test']['context']\ninstructions = test_valid['test']['instruction']\nresponses = test_valid['test']['response']\n\npeft_saved_model = PeftModel.from_pretrained(\n    pre_trained_model, \n    peft_model_paths[2], \n    torch_dtype=torch.bfloat16,\n    is_trainable=False\n).to(DEVICE)\n\npeft_model_responses = []\nstart_time = time.time()\nfor context, instruction in zip(contexts, instructions):\n    input_ids = tokenizer(\n        prompt_template.format(context=context, instruction=instruction),\n        return_tensors='pt',\n        truncation=True,\n        max_length=512,\n    ).to(DEVICE).input_ids\n    peft_model_output = peft_saved_model.generate(\n        input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)\n    )\n    peft_model_response = tokenizer.decode(peft_model_output[0], skip_special_tokens=True)\n    peft_model_responses.append(peft_model_response)\nend_time = time.time()\nprint(f\"Time taken for inference on {test_valid['test'].num_rows} examples: {end_time - start_time}\")\npeft_model_result = rouge.compute(\n    predictions=peft_model_responses,\n    references=responses,\n    use_aggregator=True,\n    use_stemmer=True,\n)\nprint(peft_model_result)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:06:38.959326Z","iopub.execute_input":"2024-01-12T10:06:38.960182Z","iopub.status.idle":"2024-01-12T10:09:56.331851Z","shell.execute_reply.started":"2024-01-12T10:06:38.960126Z","shell.execute_reply":"2024-01-12T10:09:56.330459Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Time taken for inference on 355 examples: 195.5180356502533\n{'rouge1': 0.31707048704688745, 'rouge2': 0.1795858588607784, 'rougeL': 0.2881053060618656, 'rougeLsum': 0.28913189774986503}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let us finally look at some of the responses from the fine tuned model.","metadata":{}},{"cell_type":"code","source":"for i in range(3):\n    print(''.join(['-'] * 80))\n    print('CONTEXT:')\n    print(test_valid['test'][i]['context'])\n    print('\\n\\n')\n    print('INSTRUCTION:')\n    print(test_valid['test'][i]['instruction'])\n    print('\\n\\n')\n    print('RESPONSE:')\n    print(test_valid['test'][i]['response'])\n    print('\\n\\n')\n    print('Fine-tuned-FLAN-T5-RESPONSE:')\n    inputs = tokenizer(\n        prompt_template.format(\n            context=test_valid['test'][i]['context'],\n            instruction=test_valid['test'][i]['instruction'],\n        ),\n        return_tensors='pt',\n        truncation=True,\n        max_length=512\n    ).to(DEVICE)\n    output = tokenizer.decode(\n        pre_trained_model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=200,\n        )[0], \n        skip_special_tokens=True\n    )\n    print(output)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:22.513928Z","iopub.execute_input":"2024-01-12T10:15:22.514378Z","iopub.status.idle":"2024-01-12T10:15:28.855649Z","shell.execute_reply.started":"2024-01-12T10:15:22.514345Z","shell.execute_reply":"2024-01-12T10:15:28.853602Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"--------------------------------------------------------------------------------\nCONTEXT:\nThe Build Back Better Plan or Build Back Better agenda was a legislative framework proposed by U.S. president Joe Biden between 2020 and 2021. Generally viewed as ambitious in size and scope, it sought the largest nationwide public investment in social, infrastructural, and environmental programs since the 1930s Great Depression-era policies of the New Deal.\n\nThe Build Back Better plan was divided into three parts:\n\nAmerican Rescue Plan (ARP), a COVID-19 pandemic-relief bill;\nAmerican Jobs Plan (AJP), a proposal to address long-neglected infrastructure needs and reduce America's contributions to destructive effects of climate change; and\nAmerican Families Plan (AFP), a proposal to fund a variety of social policy initiatives, some of which (e.g., paid family leave) had never before been enacted nationally in the U.S.\nThe first part was passed as the $1.9 trillion American Rescue Plan Act of 2021, and was signed into law in March 2021. The other two parts were reworked into different bills over the course of extensive negotiations. Aspects of the AJP's infrastructure goals were diverted into the Infrastructure Investment and Jobs Act, which was signed into law in November 2021. Other AJP priorities (e.g., climate change remediation, home health care reform, etc.) were then merged with the AFP to form the Build Back Better Act. The bill passed the Democratic-controlled House of Representatives but struggled to gain the support of Democrats Joe Manchin of West Virginia and Kyrsten Sinema of Arizona in the evenly divided Senate, with unified Republican opposition. Manchin and Sinema negotiated the reduction of Build Back Better Act's size, scope, and cost significantly with Biden and Democratic congressional leaders, but Manchin, widely viewed as the key swing vote needed to pass the bill in the Senate, ultimately rejected it over the procedural tactics used. Continued negotiations between Manchin and Senate Majority Leader Chuck Schumer eventually resulted in the Inflation Reduction Act of 2022, which was signed into law in August 2022, and incorporated some of the Build Back Better Act's climate change, healthcare, and tax reform proposals while excluding its social safety net proposals.\n\n\n\nINSTRUCTION:\nGiven a reference text about the Build Back Better Plan, what are the 3 parts that make up the plan?\n\n\n\nRESPONSE:\nThe three components of the Build Back Better plan are:\n- American Rescue Plan\n- American Jobs Plan\n- American Families Plan\n\n\n\nFine-tuned-FLAN-T5-RESPONSE:\nThe Build Back Better Plan or Build Back Better agenda was a legislative framework proposed by U.S. president Joe Biden between 2020 and 2021. Generally viewed as ambitious in size and scope it sought the largest nationwide public investment in social, infrastructural, and environmental programs since the 1930s Great Depression-era policies of the New Deal. The Build Back Better plan was divided into three parts: American Rescue Plan (ARP), a COVID-19 pandemic-relief bill; American Jobs Plan (AJP), a proposal to address long-neglected infrastructure needs and reduce America's contributions to destructive effects of climate change. The American Families Plan (AFP), a proposal to fund a variety of social policy initiatives, some of which (e.g., paid family leave, etc.) were then merged with the AFP to form the Build Back Better Act. The bill passed the Democratic\n--------------------------------------------------------------------------------\nCONTEXT:\nThe Wagner Group (Russian: Группа Вагнера, tr. Gruppa Vagnera), also known as PMC Wagner (Russian: ЧВК «Вагнер», tr. ChVK «Vagner»; lit. 'Wagner Private Military Company'), is a Russian paramilitary organization. It is variously described as a private military company (PMC), a network of mercenaries, or a de facto private army of Russian President Vladimir Putin. The group operates beyond the law in Russia, where private military contractors are officially forbidden\n\n\n\nINSTRUCTION:\nWho are the Wagner group\n\n\n\nRESPONSE:\nThey are a private military organization that is endorsed by Russian President Vladimir Putin and is being used in the ongoing war against Ukraine.\n\n\n\nFine-tuned-FLAN-T5-RESPONSE:\nThe Wagner Group (Russian: руа анера, tr. ChVK «Vagner»; lit. 'Wagner Private Military Company'; lit. 'Wagner Private Military Company', is a Russian paramilitary organization. It is variously described as a private military company (PMC), a network of mercenaries, or a de facto private army of Russian President Vladimir Putin. The group operates beyond the law in Russia where private military contractors are officially forbidden. The group operates beyond the law in Russia where private military contractors are officially forbidden. The Wagner Group operates beyond the law in Russia where private military contractors are officially forbidden.\n--------------------------------------------------------------------------------\nCONTEXT:\nRraboshtë is a village located in the former Kallmet municipality, Lezhë County, northwestern Albania. At the 2015 local government reform it became part of the municipality Lezhë. The village is situated near the river Drin, about 4 km north of the city Lezhë. Nearby mountains include Mali Kastriot and Mali i Velës.\n\nIn Rraboshtë there is also a church named Te Ngjiturit e Zojës ne Qiell but also a school.\n\nRraboshta is divided into four main mahalla (\"quarter\" or \"neighbourhood\"), that of the Tatej, Pemaj, Ballije, and Mahalla e Poshtme. The majority of brotherhoods descend from Mirdita, the tribal and ethnographic region located to the east of Rraboshtë, with most having arrived from Orosh and Spaç. Examples include the Tatej (Spaçi) and Pemaj (Oroshi). However, there are other minor brotherhoods who trace their ancestry to some of the other regions of northern Albania. The Robi arrived from Dibra in north-eastern Albania while the descendants of Gjon Paloka (the Vokërr Pali) came from Kelmend in Malësia.\n\nThe villagers of Rraboshtë are known for their patriotism and for preserving the traditions and customs of the areas where they came from. The traditional dress is the kalmetore and mirditore dress. The inhabitants of this village are usually engaged in agriculture and livestock. They are famous for the production of grape varieties and the cultivation of this product.\n\n\n\nINSTRUCTION:\nGiven a reference text about Rraboshtë, tell me what the village is known for and what country it's in.\n\n\n\nRESPONSE:\nRraboshtë is located in Albania and is known for producing grape varieties.\n\n\n\nFine-tuned-FLAN-T5-RESPONSE:\nRrabosht is a village located in the former Kallmet municipality, Lezh County, northwestern Albania. At the 2015 local government reform it became part of the municipality Lezh.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}